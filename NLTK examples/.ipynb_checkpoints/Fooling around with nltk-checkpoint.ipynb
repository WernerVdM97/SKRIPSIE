{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import collections\n",
    "import unicodedata \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import inflect \n",
    "#This is a simple library for accomplishing \n",
    "#the natural language related tasks of generating plurals, \n",
    "#singular nouns, ordinals, and indefinite articles, and \n",
    "#(of most interest to us) converting numbers to words \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../corpus/bible.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.52049446105957\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pre_tokens = sent_tokenize(file)\n",
    "\n",
    "# now loop over each sentence and tokenize it separately\n",
    "for sentence in pre_tokens:\n",
    "    post_token.append(word_tokenize(sentence))\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89229\n"
     ]
    }
   ],
   "source": [
    "print(len(post_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.557124614715576\n"
     ]
    }
   ],
   "source": [
    "#normalise tokens manually\n",
    "'''\n",
    "removing non ascii\n",
    "to lowercase\n",
    "remove punctuation\n",
    "replace numbers\n",
    "remove stopwords\n",
    "'''\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    #words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for x in range(len(post_token)):\n",
    "    data.append(normalize(post_token[x]))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['in', 'the', 'beginning', 'god', 'created', 'the', 'heaven', 'and', 'the', 'earth'], ['and', 'the', 'earth', 'was', 'without', 'form', 'and', 'void', 'and', 'darkness', 'was', 'upon', 'the', 'face', 'of', 'the', 'deep'], ['and', 'the', 'spirit', 'of', 'god', 'moved', 'upon', 'the', 'face', 'of', 'the', 'waters']]\n"
     ]
    }
   ],
   "source": [
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outdated: applicable if data is tokenized entirely, not per sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-561ce2fe07b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbiF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConditionalFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbiF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconditions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/probability.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cond_samples)\u001b[0m\n\u001b[1;32m   1864\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcond_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcond_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1866\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1868\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "bigrams = ngrams(data,2)\n",
    "\n",
    "biF=nltk.ConditionalFreqDist(bigrams)\n",
    "\n",
    "biF.conditions()\n",
    "biF.keys()\n",
    "print(biF['my'].most_common(10))\n",
    "\n",
    "biP = nltk.ConditionalProbDist(biF , nltk.MLEProbDist)\n",
    "print(biP['my'].prob('people')) #prob for people given previous word is my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-d7c1f40c298b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtriF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrigrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtriF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/probability.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \"\"\"\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mCounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Cached number of samples in this FreqDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/collections/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/probability.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \"\"\"\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/collections/__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    620\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fast path when counter is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m                 \u001b[0m_count_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "trigrams = ngrams(data,3)\n",
    "\n",
    "triF = nltk.FreqDist(trigrams)\n",
    "\n",
    "triF.plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(triF.freq(('of','the','lord'))) #relative frequency estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "smoothing is required for values where answer above is equal to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(triF.N()) #total trigrams in training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good turing intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For unseen ngrams:\n",
    "$$ C_0\\cdot q_0 = \\frac{C_1}{N_\\sum} $$\n",
    "\n",
    "where $ C_0\\cdot q_0 $ is the probability of occurence of any unseen n-gram. $C_1$ is the number of n-grams occuring one time in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK LM intro, important to note that lm required data to be tokenized per sentence\n",
    "3 main steps: Building vocab, training(counting ngrams), output prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tips\n",
    ">>> import nltk\n",
    ">>> obs = 'the rain in spain falls mainly in the plains'.split()\n",
    ">>> lm = nltk.NgramModel(2, obs, estimator=nltk.MLEProbDist)\n",
    ">>> lm.prob('rain', 'the') #wrong\n",
    "0.0\n",
    ">>> lm.prob('rain', ['the']) #right\n",
    "0.5\n",
    ">>> lm.prob('spain', 'rain in') #wrong\n",
    "0.0\n",
    ">>> lm.prob('spain', ['rain in']) #wrong\n",
    "'''long exception'''\n",
    ">>> lm.prob('spain', ['rain', 'in']) #right\n",
    "1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package nltk.lm in nltk:\n",
      "\n",
      "NAME\n",
      "    nltk.lm\n",
      "\n",
      "DESCRIPTION\n",
      "    NLTK Language Modeling Module.\n",
      "    ------------------------------\n",
      "    \n",
      "    Currently this module covers only ngram language models, but it should be easy\n",
      "    to extend to neural models.\n",
      "    \n",
      "    \n",
      "    Preparing Data\n",
      "    ==============\n",
      "    \n",
      "    Before we train our ngram models it is necessary to make sure the data we put in\n",
      "    them is in the right format.\n",
      "    Let's say we have a text that is a list of sentences, where each sentence is\n",
      "    a list of strings. For simplicity we just consider a text consisting of\n",
      "    characters instead of words.\n",
      "    \n",
      "        >>> text = [['a', 'b', 'c'], ['a', 'c', 'd', 'c', 'e', 'f']]\n",
      "    \n",
      "    If we want to train a bigram model, we need to turn this text into bigrams.\n",
      "    Here's what the first sentence of our text would look like if we use a function\n",
      "    from NLTK for this.\n",
      "    \n",
      "        >>> from nltk.util import bigrams\n",
      "        >>> list(bigrams(text[0]))\n",
      "        [('a', 'b'), ('b', 'c')]\n",
      "    \n",
      "    Notice how \"b\" occurs both as the first and second member of different bigrams\n",
      "    but \"a\" and \"c\" don't? Wouldn't it be nice to somehow indicate how often sentences\n",
      "    start with \"a\" and end with \"c\"?\n",
      "    A standard way to deal with this is to add special \"padding\" symbols to the\n",
      "    sentence before splitting it into ngrams.\n",
      "    Fortunately, NLTK also has a function for that, let's see what it does to the\n",
      "    first sentence.\n",
      "    \n",
      "        >>> from nltk.util import pad_sequence\n",
      "        >>> list(pad_sequence(text[0],\n",
      "        ... pad_left=True,\n",
      "        ... left_pad_symbol=\"<s>\",\n",
      "        ... pad_right=True,\n",
      "        ... right_pad_symbol=\"</s>\",\n",
      "        ... n=2))\n",
      "        ['<s>', 'a', 'b', 'c', '</s>']\n",
      "    \n",
      "    Note the `n` argument, that tells the function we need padding for bigrams.\n",
      "    Now, passing all these parameters every time is tedious and in most cases they\n",
      "    can be safely assumed as defaults anyway.\n",
      "    Thus our module provides a convenience function that has all these arguments\n",
      "    already set while the other arguments remain the same as for `pad_sequence`.\n",
      "    \n",
      "        >>> from nltk.lm.preprocessing import pad_both_ends\n",
      "        >>> list(pad_both_ends(text[0], n=2))\n",
      "        ['<s>', 'a', 'b', 'c', '</s>']\n",
      "    \n",
      "    Combining the two parts discussed so far we get the following preparation steps\n",
      "    for one sentence.\n",
      "    \n",
      "        >>> list(bigrams(pad_both_ends(text[0], n=2)))\n",
      "        [('<s>', 'a'), ('a', 'b'), ('b', 'c'), ('c', '</s>')]\n",
      "    \n",
      "    To make our model more robust we could also train it on unigrams (single words)\n",
      "    as well as bigrams, its main source of information.\n",
      "    NLTK once again helpfully provides a function called `everygrams`.\n",
      "    While not the most efficient, it is conceptually simple.\n",
      "    \n",
      "    \n",
      "        >>> from nltk.util import everygrams\n",
      "        >>> padded_bigrams = list(pad_both_ends(text[0], n=2))\n",
      "        >>> list(everygrams(padded_bigrams, max_len=2))\n",
      "        [('<s>',),\n",
      "         ('a',),\n",
      "         ('b',),\n",
      "         ('c',),\n",
      "         ('</s>',),\n",
      "         ('<s>', 'a'),\n",
      "         ('a', 'b'),\n",
      "         ('b', 'c'),\n",
      "         ('c', '</s>')]\n",
      "    \n",
      "    We are almost ready to start counting ngrams, just one more step left.\n",
      "    During training and evaluation our model will rely on a vocabulary that\n",
      "    defines which words are \"known\" to the model.\n",
      "    To create this vocabulary we need to pad our sentences (just like for counting\n",
      "    ngrams) and then combine the sentences into one flat stream of words.\n",
      "    \n",
      "        >>> from nltk.lm.preprocessing import flatten\n",
      "        >>> list(flatten(pad_both_ends(sent, n=2) for sent in text))\n",
      "        ['<s>', 'a', 'b', 'c', '</s>', '<s>', 'a', 'c', 'd', 'c', 'e', 'f', '</s>']\n",
      "    \n",
      "    In most cases we want to use the same text as the source for both vocabulary\n",
      "    and ngram counts.\n",
      "    Now that we understand what this means for our preprocessing, we can simply import\n",
      "    a function that does everything for us.\n",
      "    \n",
      "        >>> from nltk.lm.preprocessing import padded_everygram_pipeline\n",
      "        >>> train, vocab = padded_everygram_pipeline(2, text)\n",
      "    \n",
      "    So as to avoid re-creating the text in memory, both `train` and `vocab` are lazy\n",
      "    iterators. They are evaluated on demand at training time.\n",
      "    \n",
      "    \n",
      "    Training\n",
      "    ========\n",
      "    Having prepared our data we are ready to start training a model.\n",
      "    As a simple example, let us train a Maximum Likelihood Estimator (MLE).\n",
      "    We only need to specify the highest ngram order to instantiate it.\n",
      "    \n",
      "        >>> from nltk.lm import MLE\n",
      "        >>> lm = MLE(2)\n",
      "    \n",
      "    This automatically creates an empty vocabulary...\n",
      "    \n",
      "        >>> len(lm.vocab)\n",
      "        0\n",
      "    \n",
      "    ... which gets filled as we fit the model.\n",
      "    \n",
      "        >>> lm.fit(train, vocab)\n",
      "        >>> print(lm.vocab)\n",
      "        <Vocabulary with cutoff=1 unk_label='<UNK>' and 9 items>\n",
      "        >>> len(lm.vocab)\n",
      "        9\n",
      "    \n",
      "    The vocabulary helps us handle words that have not occurred during training.\n",
      "    \n",
      "        >>> lm.vocab.lookup(text[0])\n",
      "        ('a', 'b', 'c')\n",
      "        >>> lm.vocab.lookup([\"aliens\", \"from\", \"Mars\"])\n",
      "        ('<UNK>', '<UNK>', '<UNK>')\n",
      "    \n",
      "    Moreover, in some cases we want to ignore words that we did see during training\n",
      "    but that didn't occur frequently enough, to provide us useful information.\n",
      "    You can tell the vocabulary to ignore such words.\n",
      "    To find out how that works, check out the docs for the `Vocabulary` class.\n",
      "    \n",
      "    \n",
      "    Using a Trained Model\n",
      "    =====================\n",
      "    When it comes to ngram models the training boils down to counting up the ngrams\n",
      "    from the training corpus.\n",
      "    \n",
      "        >>> print(lm.counts)\n",
      "        <NgramCounter with 2 ngram orders and 24 ngrams>\n",
      "    \n",
      "    This provides a convenient interface to access counts for unigrams...\n",
      "    \n",
      "        >>> lm.counts['a']\n",
      "        2\n",
      "    \n",
      "    ...and bigrams (in this case \"a b\")\n",
      "    \n",
      "        >>> lm.counts[['a']]['b']\n",
      "        1\n",
      "    \n",
      "    And so on. However, the real purpose of training a language model is to have it\n",
      "    score how probable words are in certain contexts.\n",
      "    This being MLE, the model returns the item's relative frequency as its score.\n",
      "    \n",
      "        >>> lm.score(\"a\")\n",
      "        0.15384615384615385\n",
      "    \n",
      "    Items that are not seen during training are mapped to the vocabulary's\n",
      "    \"unknown label\" token. This is \"<UNK>\" by default.\n",
      "    \n",
      "        >>> lm.score(\"<UNK>\") == lm.score(\"aliens\")\n",
      "        True\n",
      "    \n",
      "    Here's how you get the score for a word given some preceding context.\n",
      "    For example we want to know what is the chance that \"b\" is preceded by \"a\".\n",
      "    \n",
      "        >>> lm.score(\"b\", [\"a\"])\n",
      "        0.5\n",
      "    \n",
      "    To avoid underflow when working with many small score values it makes sense to\n",
      "    take their logarithm.\n",
      "    For convenience this can be done with the `logscore` method.\n",
      "    \n",
      "        >>> lm.logscore(\"a\")\n",
      "        -2.700439718141092\n",
      "    \n",
      "    Building on this method, we can also evaluate our model's cross-entropy and\n",
      "    perplexity with respect to sequences of ngrams.\n",
      "    \n",
      "        >>> test = [('a', 'b'), ('c', 'd')]\n",
      "        >>> lm.entropy(test)\n",
      "        1.292481250360578\n",
      "        >>> lm.perplexity(test)\n",
      "        2.449489742783178\n",
      "    \n",
      "    It is advisable to preprocess your test text exactly the same way as you did\n",
      "    the training text.\n",
      "    \n",
      "    One cool feature of ngram models is that they can be used to generate text.\n",
      "    \n",
      "        >>> lm.generate(1, random_seed=3)\n",
      "        '<s>'\n",
      "        >>> lm.generate(5, random_seed=3)\n",
      "        ['<s>', 'a', 'b', 'c', 'd']\n",
      "    \n",
      "    Provide `random_seed` if you want to consistently reproduce the same text all\n",
      "    other things being equal. Here we are using it to test the examples.\n",
      "    \n",
      "    You can also condition your generation on some preceding text with the `context`\n",
      "    argument.\n",
      "    \n",
      "        >>> lm.generate(5, text_seed=['c'], random_seed=3)\n",
      "        ['</s>', 'c', 'd', 'c', 'd']\n",
      "    \n",
      "    Note that an ngram model is restricted in how much preceding context it can\n",
      "    take into account. For example, a trigram model can only condition its output\n",
      "    on 2 preceding words. If you pass in a 4-word context, the first two words\n",
      "    will be ignored.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    api\n",
      "    counter\n",
      "    models\n",
      "    preprocessing\n",
      "    smoothing\n",
      "    util\n",
      "    vocabulary\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        nltk.lm.counter.NgramCounter\n",
      "        nltk.lm.vocabulary.Vocabulary\n",
      "    nltk.lm.api.LanguageModel(builtins.object)\n",
      "        nltk.lm.models.Lidstone\n",
      "            nltk.lm.models.Laplace\n",
      "        nltk.lm.models.MLE\n",
      "    nltk.lm.models.InterpolatedLanguageModel(nltk.lm.api.LanguageModel)\n",
      "        nltk.lm.models.KneserNeyInterpolated\n",
      "        nltk.lm.models.WittenBellInterpolated\n",
      "    \n",
      "    class KneserNeyInterpolated(InterpolatedLanguageModel)\n",
      "     |  Interpolated version of Kneser-Ney smoothing.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KneserNeyInterpolated\n",
      "     |      InterpolatedLanguageModel\n",
      "     |      nltk.lm.api.LanguageModel\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, order, discount=0.1, **kwargs)\n",
      "     |      Creates new LanguageModel.\n",
      "     |      \n",
      "     |      :param vocabulary: If provided, this vocabulary will be used instead\n",
      "     |      of creating a new one when training.\n",
      "     |      :type vocabulary: `nltk.lm.Vocabulary` or None\n",
      "     |      :param counter: If provided, use this object to count ngrams.\n",
      "     |      :type vocabulary: `nltk.lm.NgramCounter` or None\n",
      "     |      :param ngrams_fn: If given, defines how sentences in training text are turned to ngram\n",
      "     |                        sequences.\n",
      "     |      :type ngrams_fn: function or None\n",
      "     |      :param pad_fn: If given, defines how senteces in training text are padded.\n",
      "     |      :type pad_fn: function or None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from InterpolatedLanguageModel:\n",
      "     |  \n",
      "     |  unmasked_score(self, word, context=None)\n",
      "     |      Score a word given some optional context.\n",
      "     |      \n",
      "     |      Concrete models are expected to provide an implementation.\n",
      "     |      Note that this method does not mask its arguments with the OOV label.\n",
      "     |      Use the `score` method for that.\n",
      "     |      \n",
      "     |      :param str word: Word for which we want the score\n",
      "     |      :param tuple(str) context: Context the word is in.\n",
      "     |      If `None`, compute unigram score.\n",
      "     |      :param context: tuple(str) or None\n",
      "     |      :rtype: float\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.lm.api.LanguageModel:\n",
      "     |  \n",
      "     |  context_counts(self, context)\n",
      "     |      Helper method for retrieving counts for a given context.\n",
      "     |      \n",
      "     |      Assumes context has been checked and oov words in it masked.\n",
      "     |      :type context: tuple(str) or None\n",
      "     |  \n",
      "     |  entropy(self, text_ngrams)\n",
      "     |      Calculate cross-entropy of model for given evaluation text.\n",
      "     |      \n",
      "     |      :param Iterable(tuple(str)) text_ngrams: A sequence of ngram tuples.\n",
      "     |      :rtype: float\n",
      "     |  \n",
      "     |  fit(self, text, vocabulary_text=None)\n",
      "     |      Trains the model on a text.\n",
      "     |      \n",
      "     |      :param text: Training text as a sequence of sentences.\n",
      "     |  \n",
      "     |  generate(self, num_words=1, text_seed=None, random_seed=None)\n",
      "     |      Generate words from the model.\n",
      "     |      \n",
      "     |      :param int num_words: How many words to generate. By default 1.\n",
      "     |      :param text_seed: Generation can be conditioned on preceding context.\n",
      "     |      :param random_seed: A random seed or an instance of `random.Random`. If provided,\n",
      "     |      makes the random sampling part of generation reproducible.\n",
      "     |      :return: One (str) word or a list of words generated from model.\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |      \n",
      "     |      >>> from nltk.lm import MLE\n",
      "     |      >>> lm = MLE(2)\n",
      "     |      >>> lm.fit([[(\"a\", \"b\"), (\"b\", \"c\")]], vocabulary_text=['a', 'b', 'c'])\n",
      "     |      >>> lm.fit([[(\"a\",), (\"b\",), (\"c\",)]])\n",
      "     |      >>> lm.generate(random_seed=3)\n",
      "     |      'a'\n",
      "     |      >>> lm.generate(text_seed=['a'])\n",
      "     |      'b'\n",
      "     |  \n",
      "     |  logscore(self, word, context=None)\n",
      "     |      Evaluate the log score of this word in this context.\n",
      "     |      \n",
      "     |      The arguments are the same as for `score` and `unmasked_score`.\n",
      "     |  \n",
      "     |  perplexity(self, text_ngrams)\n",
      "     |      Calculates the perplexity of the given text.\n",
      "     |      \n",
      "     |      This is simply 2 ** cross-entropy for the text, so the arguments are the same.\n",
      "     |  \n",
      "     |  score(self, word, context=None)\n",
      "     |      Masks out of vocab (OOV) words and computes their model score.\n",
      "     |      \n",
      "     |      For model-specific logic of calculating scores, see the `unmasked_score`\n",
      "     |      method.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.lm.api.LanguageModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Laplace(Lidstone)\n",
      "     |  Implements Laplace (add one) smoothing.\n",
      "     |  \n",
      "     |  Initialization identical to BaseNgramModel because gamma is always 1.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Laplace\n",
      "     |      Lidstone\n",
      "     |      nltk.lm.api.LanguageModel\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *args, **kwargs)\n",
      "     |      Creates new LanguageModel.\n",
      "     |      \n",
      "     |      :param vocabulary: If provided, this vocabulary will be used instead\n",
      "     |      of creating a new one when training.\n",
      "     |      :type vocabulary: `nltk.lm.Vocabulary` or None\n",
      "     |      :param counter: If provided, use this object to count ngrams.\n",
      "     |      :type vocabulary: `nltk.lm.NgramCounter` or None\n",
      "     |      :param ngrams_fn: If given, defines how sentences in training text are turned to ngram\n",
      "     |                        sequences.\n",
      "     |      :type ngrams_fn: function or None\n",
      "     |      :param pad_fn: If given, defines how senteces in training text are padded.\n",
      "     |      :type pad_fn: function or None\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self, /)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Lidstone:\n",
      "     |  \n",
      "     |  unmasked_score(self, word, context=None)\n",
      "     |      Add-one smoothing: Lidstone or Laplace.\n",
      "     |      \n",
      "     |      To see what kind, look at `gamma` attribute on the class.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.lm.api.LanguageModel:\n",
      "     |  \n",
      "     |  context_counts(self, context)\n",
      "     |      Helper method for retrieving counts for a given context.\n",
      "     |      \n",
      "     |      Assumes context has been checked and oov words in it masked.\n",
      "     |      :type context: tuple(str) or None\n",
      "     |  \n",
      "     |  entropy(self, text_ngrams)\n",
      "     |      Calculate cross-entropy of model for given evaluation text.\n",
      "     |      \n",
      "     |      :param Iterable(tuple(str)) text_ngrams: A sequence of ngram tuples.\n",
      "     |      :rtype: float\n",
      "     |  \n",
      "     |  fit(self, text, vocabulary_text=None)\n",
      "     |      Trains the model on a text.\n",
      "     |      \n",
      "     |      :param text: Training text as a sequence of sentences.\n",
      "     |  \n",
      "     |  generate(self, num_words=1, text_seed=None, random_seed=None)\n",
      "     |      Generate words from the model.\n",
      "     |      \n",
      "     |      :param int num_words: How many words to generate. By default 1.\n",
      "     |      :param text_seed: Generation can be conditioned on preceding context.\n",
      "     |      :param random_seed: A random seed or an instance of `random.Random`. If provided,\n",
      "     |      makes the random sampling part of generation reproducible.\n",
      "     |      :return: One (str) word or a list of words generated from model.\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |      \n",
      "     |      >>> from nltk.lm import MLE\n",
      "     |      >>> lm = MLE(2)\n",
      "     |      >>> lm.fit([[(\"a\", \"b\"), (\"b\", \"c\")]], vocabulary_text=['a', 'b', 'c'])\n",
      "     |      >>> lm.fit([[(\"a\",), (\"b\",), (\"c\",)]])\n",
      "     |      >>> lm.generate(random_seed=3)\n",
      "     |      'a'\n",
      "     |      >>> lm.generate(text_seed=['a'])\n",
      "     |      'b'\n",
      "     |  \n",
      "     |  logscore(self, word, context=None)\n",
      "     |      Evaluate the log score of this word in this context.\n",
      "     |      \n",
      "     |      The arguments are the same as for `score` and `unmasked_score`.\n",
      "     |  \n",
      "     |  perplexity(self, text_ngrams)\n",
      "     |      Calculates the perplexity of the given text.\n",
      "     |      \n",
      "     |      This is simply 2 ** cross-entropy for the text, so the arguments are the same.\n",
      "     |  \n",
      "     |  score(self, word, context=None)\n",
      "     |      Masks out of vocab (OOV) words and computes their model score.\n",
      "     |      \n",
      "     |      For model-specific logic of calculating scores, see the `unmasked_score`\n",
      "     |      method.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.lm.api.LanguageModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Lidstone(nltk.lm.api.LanguageModel)\n",
      "     |  Provides Lidstone-smoothed scores.\n",
      "     |  \n",
      "     |  In addition to initialization arguments from BaseNgramModel also requires\n",
      "     |  a number by which to increase the counts, gamma.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Lidstone\n",
      "     |      nltk.lm.api.LanguageModel\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, gamma, *args, **kwargs)\n",
      "     |      Creates new LanguageModel.\n",
      "     |      \n",
      "     |      :param vocabulary: If provided, this vocabulary will be used instead\n",
      "     |      of creating a new one when training.\n",
      "     |      :type vocabulary: `nltk.lm.Vocabulary` or None\n",
      "     |      :param counter: If provided, use this object to count ngrams.\n",
      "     |      :type vocabulary: `nltk.lm.NgramCounter` or None\n",
      "     |      :param ngrams_fn: If given, defines how sentences in training text are turned to ngram\n",
      "     |                        sequences.\n",
      "     |      :type ngrams_fn: function or None\n",
      "     |      :param pad_fn: If given, defines how senteces in training text are padded.\n",
      "     |      :type pad_fn: function or None\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self, /)\n",
      "     |  \n",
      "     |  unmasked_score(self, word, context=None)\n",
      "     |      Add-one smoothing: Lidstone or Laplace.\n",
      "     |      \n",
      "     |      To see what kind, look at `gamma` attribute on the class.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.lm.api.LanguageModel:\n",
      "     |  \n",
      "     |  context_counts(self, context)\n",
      "     |      Helper method for retrieving counts for a given context.\n",
      "     |      \n",
      "     |      Assumes context has been checked and oov words in it masked.\n",
      "     |      :type context: tuple(str) or None\n",
      "     |  \n",
      "     |  entropy(self, text_ngrams)\n",
      "     |      Calculate cross-entropy of model for given evaluation text.\n",
      "     |      \n",
      "     |      :param Iterable(tuple(str)) text_ngrams: A sequence of ngram tuples.\n",
      "     |      :rtype: float\n",
      "     |  \n",
      "     |  fit(self, text, vocabulary_text=None)\n",
      "     |      Trains the model on a text.\n",
      "     |      \n",
      "     |      :param text: Training text as a sequence of sentences.\n",
      "     |  \n",
      "     |  generate(self, num_words=1, text_seed=None, random_seed=None)\n",
      "     |      Generate words from the model.\n",
      "     |      \n",
      "     |      :param int num_words: How many words to generate. By default 1.\n",
      "     |      :param text_seed: Generation can be conditioned on preceding context.\n",
      "     |      :param random_seed: A random seed or an instance of `random.Random`. If provided,\n",
      "     |      makes the random sampling part of generation reproducible.\n",
      "     |      :return: One (str) word or a list of words generated from model.\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |      \n",
      "     |      >>> from nltk.lm import MLE\n",
      "     |      >>> lm = MLE(2)\n",
      "     |      >>> lm.fit([[(\"a\", \"b\"), (\"b\", \"c\")]], vocabulary_text=['a', 'b', 'c'])\n",
      "     |      >>> lm.fit([[(\"a\",), (\"b\",), (\"c\",)]])\n",
      "     |      >>> lm.generate(random_seed=3)\n",
      "     |      'a'\n",
      "     |      >>> lm.generate(text_seed=['a'])\n",
      "     |      'b'\n",
      "     |  \n",
      "     |  logscore(self, word, context=None)\n",
      "     |      Evaluate the log score of this word in this context.\n",
      "     |      \n",
      "     |      The arguments are the same as for `score` and `unmasked_score`.\n",
      "     |  \n",
      "     |  perplexity(self, text_ngrams)\n",
      "     |      Calculates the perplexity of the given text.\n",
      "     |      \n",
      "     |      This is simply 2 ** cross-entropy for the text, so the arguments are the same.\n",
      "     |  \n",
      "     |  score(self, word, context=None)\n",
      "     |      Masks out of vocab (OOV) words and computes their model score.\n",
      "     |      \n",
      "     |      For model-specific logic of calculating scores, see the `unmasked_score`\n",
      "     |      method.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.lm.api.LanguageModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MLE(nltk.lm.api.LanguageModel)\n",
      "     |  Class for providing MLE ngram model scores.\n",
      "     |  \n",
      "     |  Inherits initialization from BaseNgramModel.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MLE\n",
      "     |      nltk.lm.api.LanguageModel\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self, /)\n",
      "     |  \n",
      "     |  unmasked_score(self, word, context=None)\n",
      "     |      Returns the MLE score for a word given a context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |      - word is expcected to be a string\n",
      "     |      - context is expected to be something reasonably convertible to a tuple\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.lm.api.LanguageModel:\n",
      "     |  \n",
      "     |  __init__(self, order, vocabulary=None, counter=None)\n",
      "     |      Creates new LanguageModel.\n",
      "     |      \n",
      "     |      :param vocabulary: If provided, this vocabulary will be used instead\n",
      "     |      of creating a new one when training.\n",
      "     |      :type vocabulary: `nltk.lm.Vocabulary` or None\n",
      "     |      :param counter: If provided, use this object to count ngrams.\n",
      "     |      :type vocabulary: `nltk.lm.NgramCounter` or None\n",
      "     |      :param ngrams_fn: If given, defines how sentences in training text are turned to ngram\n",
      "     |                        sequences.\n",
      "     |      :type ngrams_fn: function or None\n",
      "     |      :param pad_fn: If given, defines how senteces in training text are padded.\n",
      "     |      :type pad_fn: function or None\n",
      "     |  \n",
      "     |  context_counts(self, context)\n",
      "     |      Helper method for retrieving counts for a given context.\n",
      "     |      \n",
      "     |      Assumes context has been checked and oov words in it masked.\n",
      "     |      :type context: tuple(str) or None\n",
      "     |  \n",
      "     |  entropy(self, text_ngrams)\n",
      "     |      Calculate cross-entropy of model for given evaluation text.\n",
      "     |      \n",
      "     |      :param Iterable(tuple(str)) text_ngrams: A sequence of ngram tuples.\n",
      "     |      :rtype: float\n",
      "     |  \n",
      "     |  fit(self, text, vocabulary_text=None)\n",
      "     |      Trains the model on a text.\n",
      "     |      \n",
      "     |      :param text: Training text as a sequence of sentences.\n",
      "     |  \n",
      "     |  generate(self, num_words=1, text_seed=None, random_seed=None)\n",
      "     |      Generate words from the model.\n",
      "     |      \n",
      "     |      :param int num_words: How many words to generate. By default 1.\n",
      "     |      :param text_seed: Generation can be conditioned on preceding context.\n",
      "     |      :param random_seed: A random seed or an instance of `random.Random`. If provided,\n",
      "     |      makes the random sampling part of generation reproducible.\n",
      "     |      :return: One (str) word or a list of words generated from model.\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |      \n",
      "     |      >>> from nltk.lm import MLE\n",
      "     |      >>> lm = MLE(2)\n",
      "     |      >>> lm.fit([[(\"a\", \"b\"), (\"b\", \"c\")]], vocabulary_text=['a', 'b', 'c'])\n",
      "     |      >>> lm.fit([[(\"a\",), (\"b\",), (\"c\",)]])\n",
      "     |      >>> lm.generate(random_seed=3)\n",
      "     |      'a'\n",
      "     |      >>> lm.generate(text_seed=['a'])\n",
      "     |      'b'\n",
      "     |  \n",
      "     |  logscore(self, word, context=None)\n",
      "     |      Evaluate the log score of this word in this context.\n",
      "     |      \n",
      "     |      The arguments are the same as for `score` and `unmasked_score`.\n",
      "     |  \n",
      "     |  perplexity(self, text_ngrams)\n",
      "     |      Calculates the perplexity of the given text.\n",
      "     |      \n",
      "     |      This is simply 2 ** cross-entropy for the text, so the arguments are the same.\n",
      "     |  \n",
      "     |  score(self, word, context=None)\n",
      "     |      Masks out of vocab (OOV) words and computes their model score.\n",
      "     |      \n",
      "     |      For model-specific logic of calculating scores, see the `unmasked_score`\n",
      "     |      method.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.lm.api.LanguageModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class NgramCounter(builtins.object)\n",
      "     |  Class for counting ngrams.\n",
      "     |  \n",
      "     |  Will count any ngram sequence you give it ;)\n",
      "     |  \n",
      "     |  First we need to make sure we are feeding the counter sentences of ngrams.\n",
      "     |  \n",
      "     |  >>> text = [[\"a\", \"b\", \"c\", \"d\"], [\"a\", \"c\", \"d\", \"c\"]]\n",
      "     |  >>> from nltk.util import ngrams\n",
      "     |  >>> text_bigrams = [ngrams(sent, 2) for sent in text]\n",
      "     |  >>> text_unigrams = [ngrams(sent, 1) for sent in text]\n",
      "     |  \n",
      "     |  The counting itself is very simple.\n",
      "     |  \n",
      "     |  >>> from nltk.lm import NgramCounter\n",
      "     |  >>> ngram_counts = NgramCounter(text_bigrams + text_unigrams)\n",
      "     |  \n",
      "     |  You can conveniently access ngram counts using standard python dictionary notation.\n",
      "     |  String keys will give you unigram counts.\n",
      "     |  \n",
      "     |  >>> ngram_counts['a']\n",
      "     |  2\n",
      "     |  >>> ngram_counts['aliens']\n",
      "     |  0\n",
      "     |  \n",
      "     |  If you want to access counts for higher order ngrams, use a list or a tuple.\n",
      "     |  These are treated as \"context\" keys, so what you get is a frequency distribution\n",
      "     |  over all continuations after the given context.\n",
      "     |  \n",
      "     |  >>> sorted(ngram_counts[['a']].items())\n",
      "     |  [('b', 1), ('c', 1)]\n",
      "     |  >>> sorted(ngram_counts[('a',)].items())\n",
      "     |  [('b', 1), ('c', 1)]\n",
      "     |  \n",
      "     |  This is equivalent to specifying explicitly the order of the ngram (in this case\n",
      "     |  2 for bigram) and indexing on the context.\n",
      "     |  >>> ngram_counts[2][('a',)] is ngram_counts[['a']]\n",
      "     |  True\n",
      "     |  \n",
      "     |  Note that the keys in `ConditionalFreqDist` cannot be lists, only tuples!\n",
      "     |  It is generally advisable to use the less verbose and more flexible square\n",
      "     |  bracket notation.\n",
      "     |  \n",
      "     |  To get the count of the full ngram \"a b\", do this:\n",
      "     |  \n",
      "     |  >>> ngram_counts[['a']]['b']\n",
      "     |  1\n",
      "     |  \n",
      "     |  Specifying the ngram order as a number can be useful for accessing all ngrams\n",
      "     |  in that order.\n",
      "     |  \n",
      "     |  >>> ngram_counts[2]\n",
      "     |  <ConditionalFreqDist with 4 conditions>\n",
      "     |  \n",
      "     |  The keys of this `ConditionalFreqDist` are the contexts we discussed earlier.\n",
      "     |  Unigrams can also be accessed with a human-friendly alias.\n",
      "     |  \n",
      "     |  >>> ngram_counts.unigrams is ngram_counts[1]\n",
      "     |  True\n",
      "     |  \n",
      "     |  Similarly to `collections.Counter`, you can update counts after initialization.\n",
      "     |  \n",
      "     |  >>> ngram_counts['e']\n",
      "     |  0\n",
      "     |  >>> ngram_counts.update([ngrams([\"d\", \"e\", \"f\"], 1)])\n",
      "     |  >>> ngram_counts['e']\n",
      "     |  1\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  N(self)\n",
      "     |      Returns grand total number of ngrams stored.\n",
      "     |      \n",
      "     |      This includes ngrams from all orders, so some duplication is expected.\n",
      "     |      :rtype: int\n",
      "     |      \n",
      "     |      >>> from nltk.lm import NgramCounter\n",
      "     |      >>> counts = NgramCounter([[(\"a\", \"b\"), (\"c\",), (\"d\", \"e\")]])\n",
      "     |      >>> counts.N()\n",
      "     |      3\n",
      "     |  \n",
      "     |  __contains__(self, item)\n",
      "     |  \n",
      "     |  __getitem__(self, item)\n",
      "     |      User-friendly access to ngram counts.\n",
      "     |  \n",
      "     |  __init__(self, ngram_text=None)\n",
      "     |      Creates a new NgramCounter.\n",
      "     |      \n",
      "     |      If `ngram_text` is specified, counts ngrams from it, otherwise waits for\n",
      "     |      `update` method to be called explicitly.\n",
      "     |      \n",
      "     |      :param ngram_text: Optional text containing senteces of ngrams, as for `update` method.\n",
      "     |      :type ngram_text: Iterable(Iterable(tuple(str))) or None\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self)\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self, /)\n",
      "     |  \n",
      "     |  update(self, ngram_text)\n",
      "     |      Updates ngram counts from `ngram_text`.\n",
      "     |      \n",
      "     |      Expects `ngram_text` to be a sequence of sentences (sequences).\n",
      "     |      Each sentence consists of ngrams as tuples of strings.\n",
      "     |      \n",
      "     |      :param Iterable(Iterable(tuple(str))) ngram_text: Text containing senteces of ngrams.\n",
      "     |      :raises TypeError: if the ngrams are not tuples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Vocabulary(builtins.object)\n",
      "     |  Stores language model vocabulary.\n",
      "     |  \n",
      "     |  Satisfies two common language modeling requirements for a vocabulary:\n",
      "     |  - When checking membership and calculating its size, filters items\n",
      "     |    by comparing their counts to a cutoff value.\n",
      "     |  - Adds a special \"unknown\" token which unseen words are mapped to.\n",
      "     |  \n",
      "     |  >>> words = ['a', 'c', '-', 'd', 'c', 'a', 'b', 'r', 'a', 'c', 'd']\n",
      "     |  >>> from nltk.lm import Vocabulary\n",
      "     |  >>> vocab = Vocabulary(words, unk_cutoff=2)\n",
      "     |  \n",
      "     |  Tokens with counts greater than or equal to the cutoff value will\n",
      "     |  be considered part of the vocabulary.\n",
      "     |  \n",
      "     |  >>> vocab['c']\n",
      "     |  3\n",
      "     |  >>> 'c' in vocab\n",
      "     |  True\n",
      "     |  >>> vocab['d']\n",
      "     |  2\n",
      "     |  >>> 'd' in vocab\n",
      "     |  True\n",
      "     |  \n",
      "     |  Tokens with frequency counts less than the cutoff value will be considered not\n",
      "     |  part of the vocabulary even though their entries in the count dictionary are\n",
      "     |  preserved.\n",
      "     |  \n",
      "     |  >>> vocab['b']\n",
      "     |  1\n",
      "     |  >>> 'b' in vocab\n",
      "     |  False\n",
      "     |  >>> vocab['aliens']\n",
      "     |  0\n",
      "     |  >>> 'aliens' in vocab\n",
      "     |  False\n",
      "     |  \n",
      "     |  Keeping the count entries for seen words allows us to change the cutoff value\n",
      "     |  without having to recalculate the counts.\n",
      "     |  \n",
      "     |  >>> vocab2 = Vocabulary(vocab.counts, unk_cutoff=1)\n",
      "     |  >>> \"b\" in vocab2\n",
      "     |  True\n",
      "     |  \n",
      "     |  The cutoff value influences not only membership checking but also the result of\n",
      "     |  getting the size of the vocabulary using the built-in `len`.\n",
      "     |  Note that while the number of keys in the vocabulary's counter stays the same,\n",
      "     |  the items in the vocabulary differ depending on the cutoff.\n",
      "     |  We use `sorted` to demonstrate because it keeps the order consistent.\n",
      "     |  \n",
      "     |  >>> sorted(vocab2.counts)\n",
      "     |  ['-', 'a', 'b', 'c', 'd', 'r']\n",
      "     |  >>> sorted(vocab2)\n",
      "     |  ['-', '<UNK>', 'a', 'b', 'c', 'd', 'r']\n",
      "     |  >>> sorted(vocab.counts)\n",
      "     |  ['-', 'a', 'b', 'c', 'd', 'r']\n",
      "     |  >>> sorted(vocab)\n",
      "     |  ['<UNK>', 'a', 'c', 'd']\n",
      "     |  \n",
      "     |  In addition to items it gets populated with, the vocabulary stores a special\n",
      "     |  token that stands in for so-called \"unknown\" items. By default it's \"<UNK>\".\n",
      "     |  \n",
      "     |  >>> \"<UNK>\" in vocab\n",
      "     |  True\n",
      "     |  \n",
      "     |  We can look up words in a vocabulary using its `lookup` method.\n",
      "     |  \"Unseen\" words (with counts less than cutoff) are looked up as the unknown label.\n",
      "     |  If given one word (a string) as an input, this method will return a string.\n",
      "     |  \n",
      "     |  >>> vocab.lookup(\"a\")\n",
      "     |  'a'\n",
      "     |  >>> vocab.lookup(\"aliens\")\n",
      "     |  '<UNK>'\n",
      "     |  \n",
      "     |  If given a sequence, it will return an tuple of the looked up words.\n",
      "     |  \n",
      "     |  >>> vocab.lookup([\"p\", 'a', 'r', 'd', 'b', 'c'])\n",
      "     |  ('<UNK>', 'a', '<UNK>', 'd', '<UNK>', 'c')\n",
      "     |  \n",
      "     |  It's possible to update the counts after the vocabulary has been created.\n",
      "     |  The interface follows that of `collections.Counter`.\n",
      "     |  \n",
      "     |  >>> vocab['b']\n",
      "     |  1\n",
      "     |  >>> vocab.update([\"b\", \"b\", \"c\"])\n",
      "     |  >>> vocab['b']\n",
      "     |  3\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __contains__(self, item)\n",
      "     |      Only consider items with counts GE to cutoff as being in the\n",
      "     |      vocabulary.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __getitem__(self, item)\n",
      "     |  \n",
      "     |  __init__(self, counts=None, unk_cutoff=1, unk_label='<UNK>')\n",
      "     |      Create a new Vocabulary.\n",
      "     |      \n",
      "     |      :param counts: Optional iterable or `collections.Counter` instance to\n",
      "     |                     pre-seed the Vocabulary. In case it is iterable, counts\n",
      "     |                     are calculated.\n",
      "     |      :param int unk_cutoff: Words that occur less frequently than this value\n",
      "     |                             are not considered part of the vocabulary.\n",
      "     |      :param unk_label: Label for marking words not part of vocabulary.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Building on membership check define how to iterate over\n",
      "     |      vocabulary.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Computing size of vocabulary reflects the cutoff.\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self)\n",
      "     |  \n",
      "     |  lookup(self, words)\n",
      "     |      Look up one or more words in the vocabulary.\n",
      "     |      \n",
      "     |      If passed one word as a string will return that word or `self.unk_label`.\n",
      "     |      Otherwise will assume it was passed a sequence of words, will try to look\n",
      "     |      each of them up and return an iterator over the looked up words.\n",
      "     |      \n",
      "     |      :param words: Word(s) to look up.\n",
      "     |      :type words: Iterable(str) or str\n",
      "     |      :rtype: generator(str) or str\n",
      "     |      :raises: TypeError for types other than strings or iterables\n",
      "     |      \n",
      "     |      >>> from nltk.lm import Vocabulary\n",
      "     |      >>> vocab = Vocabulary([\"a\", \"b\", \"c\", \"a\", \"b\"], unk_cutoff=2)\n",
      "     |      >>> vocab.lookup(\"a\")\n",
      "     |      'a'\n",
      "     |      >>> vocab.lookup(\"aliens\")\n",
      "     |      '<UNK>'\n",
      "     |      >>> vocab.lookup([\"a\", \"b\", \"c\", [\"x\", \"b\"]])\n",
      "     |      ('a', 'b', '<UNK>', ('<UNK>', 'b'))\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self, /)\n",
      "     |  \n",
      "     |  update(self, *counter_args, **counter_kwargs)\n",
      "     |      Update vocabulary counts.\n",
      "     |      \n",
      "     |      Wraps `collections.Counter.update` method.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  cutoff\n",
      "     |      Cutoff value.\n",
      "     |      \n",
      "     |      Items with count below this value are not considered part of vocabulary.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class WittenBellInterpolated(InterpolatedLanguageModel)\n",
      "     |  Interpolated version of Witten-Bell smoothing.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      WittenBellInterpolated\n",
      "     |      InterpolatedLanguageModel\n",
      "     |      nltk.lm.api.LanguageModel\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, order, **kwargs)\n",
      "     |      Creates new LanguageModel.\n",
      "     |      \n",
      "     |      :param vocabulary: If provided, this vocabulary will be used instead\n",
      "     |      of creating a new one when training.\n",
      "     |      :type vocabulary: `nltk.lm.Vocabulary` or None\n",
      "     |      :param counter: If provided, use this object to count ngrams.\n",
      "     |      :type vocabulary: `nltk.lm.NgramCounter` or None\n",
      "     |      :param ngrams_fn: If given, defines how sentences in training text are turned to ngram\n",
      "     |                        sequences.\n",
      "     |      :type ngrams_fn: function or None\n",
      "     |      :param pad_fn: If given, defines how senteces in training text are padded.\n",
      "     |      :type pad_fn: function or None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from InterpolatedLanguageModel:\n",
      "     |  \n",
      "     |  unmasked_score(self, word, context=None)\n",
      "     |      Score a word given some optional context.\n",
      "     |      \n",
      "     |      Concrete models are expected to provide an implementation.\n",
      "     |      Note that this method does not mask its arguments with the OOV label.\n",
      "     |      Use the `score` method for that.\n",
      "     |      \n",
      "     |      :param str word: Word for which we want the score\n",
      "     |      :param tuple(str) context: Context the word is in.\n",
      "     |      If `None`, compute unigram score.\n",
      "     |      :param context: tuple(str) or None\n",
      "     |      :rtype: float\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.lm.api.LanguageModel:\n",
      "     |  \n",
      "     |  context_counts(self, context)\n",
      "     |      Helper method for retrieving counts for a given context.\n",
      "     |      \n",
      "     |      Assumes context has been checked and oov words in it masked.\n",
      "     |      :type context: tuple(str) or None\n",
      "     |  \n",
      "     |  entropy(self, text_ngrams)\n",
      "     |      Calculate cross-entropy of model for given evaluation text.\n",
      "     |      \n",
      "     |      :param Iterable(tuple(str)) text_ngrams: A sequence of ngram tuples.\n",
      "     |      :rtype: float\n",
      "     |  \n",
      "     |  fit(self, text, vocabulary_text=None)\n",
      "     |      Trains the model on a text.\n",
      "     |      \n",
      "     |      :param text: Training text as a sequence of sentences.\n",
      "     |  \n",
      "     |  generate(self, num_words=1, text_seed=None, random_seed=None)\n",
      "     |      Generate words from the model.\n",
      "     |      \n",
      "     |      :param int num_words: How many words to generate. By default 1.\n",
      "     |      :param text_seed: Generation can be conditioned on preceding context.\n",
      "     |      :param random_seed: A random seed or an instance of `random.Random`. If provided,\n",
      "     |      makes the random sampling part of generation reproducible.\n",
      "     |      :return: One (str) word or a list of words generated from model.\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |      \n",
      "     |      >>> from nltk.lm import MLE\n",
      "     |      >>> lm = MLE(2)\n",
      "     |      >>> lm.fit([[(\"a\", \"b\"), (\"b\", \"c\")]], vocabulary_text=['a', 'b', 'c'])\n",
      "     |      >>> lm.fit([[(\"a\",), (\"b\",), (\"c\",)]])\n",
      "     |      >>> lm.generate(random_seed=3)\n",
      "     |      'a'\n",
      "     |      >>> lm.generate(text_seed=['a'])\n",
      "     |      'b'\n",
      "     |  \n",
      "     |  logscore(self, word, context=None)\n",
      "     |      Evaluate the log score of this word in this context.\n",
      "     |      \n",
      "     |      The arguments are the same as for `score` and `unmasked_score`.\n",
      "     |  \n",
      "     |  perplexity(self, text_ngrams)\n",
      "     |      Calculates the perplexity of the given text.\n",
      "     |      \n",
      "     |      This is simply 2 ** cross-entropy for the text, so the arguments are the same.\n",
      "     |  \n",
      "     |  score(self, word, context=None)\n",
      "     |      Masks out of vocab (OOV) words and computes their model score.\n",
      "     |      \n",
      "     |      For model-specific logic of calculating scores, see the `unmasked_score`\n",
      "     |      method.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.lm.api.LanguageModel:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Vocabulary', 'NgramCounter', 'MLE', 'Lidstone', 'Laplace',...\n",
      "\n",
      "FILE\n",
      "    /home/werner/.local/lib/python3.6/site-packages/nltk/lm/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import lm\n",
    "help(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'b', 'c']]\n",
      "[('a', 'b'), ('b', 'c')]\n"
     ]
    }
   ],
   "source": [
    "#from nltk.model import build_vocabulary <----- outdated, package replaced\n",
    "\n",
    "text = [['a', 'b', 'c'], ['a', 'c', 'd', 'c', 'e', 'f']]\n",
    "print(text[:1])\n",
    "print(list(ngrams(text[0],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "#building vocab\n",
    "train, vocab = padded_everygram_pipeline(2, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "#training\n",
    "lm = MLE(2) #highest ngram in paramater. Instantiation creates empty vocab\n",
    "lm.fit(train, vocab)\n",
    "print(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using model\n",
    "print(lm.counts)\n",
    "\n",
    "#counting unigrams with a and displaying its score\n",
    "print(lm.counts['a'])\n",
    "print(lm.score('a'))\n",
    "print(lm.logscore('a'))\n",
    "#counting bigrams with a b\n",
    "print(lm.counts[['a']]['b'])\n",
    "\n",
    "#chance for b given previous word was a\n",
    "print(lm.score('b', ['a']))\n",
    "#or using log domain\n",
    "print(lm.logscore('b',['a']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This being MLE, the model returns the item’s relative frequency as its score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perplexity and entropy\n",
    ">>> test = [('a', 'b'), ('c', 'd')]\n",
    ">>> lm.entropy(test)\n",
    "1.292481250360578\n",
    ">>> lm.perplexity(test)\n",
    "2.449489742783178"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating random sentences\n",
    "lm.generate(8, random_seed=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply this on larger corpus still bigrams do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import lm\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempData = [['awe','hierdie','is','my','toets','data'],['ek','hoop','dit', 'werk'],['as','dit','nie','werk','nie'],['is','ek','fucked'],['so','kom','ons','hoop','vir','die','beste']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['awe', 'hierdie', 'is', 'my', 'toets', 'data'], ['ek', 'hoop', 'dit', 'werk'], ['as', 'dit', 'nie', 'werk', 'nie'], ['is', 'ek', 'fucked'], ['so', 'kom', 'ons', 'hoop', 'vir', 'die', 'beste'], ['hos', 'kom', 'ons', 'kyk']]\n"
     ]
    }
   ],
   "source": [
    "tempData.append(['hos','kom','ons','kyk'])\n",
    "print(tempData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In the beginning God created the heaven and the earth.', 'And the earth was without form, and void; and darkness was upon the face of the deep.', 'And the Spirit of God moved upon the face of the waters.']\n"
     ]
    }
   ],
   "source": [
    "print(pre_tokens[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.'], ['And', 'the', 'earth', 'was', 'without', 'form', ',', 'and', 'void', ';', 'and', 'darkness', 'was', 'upon', 'the', 'face', 'of', 'the', 'deep', '.'], ['And', 'the', 'Spirit', 'of', 'God', 'moved', 'upon', 'the', 'face', 'of', 'the', 'waters', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(post_token[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['in', 'the', 'beginning', 'god', 'created', 'the', 'heaven', 'and', 'the', 'earth'], ['and', 'the', 'earth', 'was', 'without', 'form', 'and', 'void', 'and', 'darkness', 'was', 'upon', 'the', 'face', 'of', 'the', 'deep'], ['and', 'the', 'spirit', 'of', 'god', 'moved', 'upon', 'the', 'face', 'of', 'the', 'waters']]\n"
     ]
    }
   ],
   "source": [
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('in', 'the'), ('the', 'beginning'), ('beginning', 'god'), ('god', 'created'), ('created', 'the'), ('the', 'heaven'), ('heaven', 'and'), ('and', 'the'), ('the', 'earth')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(data[0],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2, vocab2 = padded_everygram_pipeline(2,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.306317806243896\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "start = time.time()\n",
    "lm2 = MLE(2) #highest ngram in paramater. Instantiation creates empty vocab\n",
    "lm2.fit(train2, vocab2)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chief',\n",
       " 'baker',\n",
       " 'had',\n",
       " 'by',\n",
       " 'cities',\n",
       " 'of',\n",
       " 'these',\n",
       " 'three',\n",
       " 'thousand',\n",
       " 'and',\n",
       " 'prepare',\n",
       " 'him',\n",
       " 'all',\n",
       " 'his',\n",
       " 'eyes']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.generate(15, random_seed=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 12564 items>\n"
     ]
    }
   ],
   "source": [
    "print(lm2.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets try trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3, vocab3 = padded_everygram_pipeline(3,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.633129835128784\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "start = time.time()\n",
    "lm3 = MLE(3) #highest ngram in paramater. Instantiation creates empty vocab\n",
    "lm3.fit(train3, vocab3)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where', 'we', 'abode', 'in', 'the', 'second', 'time', 'and', 'that', 'they']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm3.generate(10, random_seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 12564 items>\n",
      "<NgramCounter with 3 ngram orders and 7927398 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(lm3.vocab)\n",
    "print(lm3.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finding distributions, however recalculates ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import everygrams\n",
    "from nltk.lm import NgramCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.043359279632568\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f553c23e160>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "tempdata = [item for sublist in data for item in sublist] #flatten list\n",
    "all_ngrams = everygrams(tempdata,max_len=3)\n",
    "fd = nltk.FreqDist(all_ngrams)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "fd.plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.658041000366211\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "text_bigrams = [ngrams(sent, 2) for sent in data]\n",
    "text_unigrams = [ngrams(sent, 1) for sent in data]\n",
    "text_trigrams = [ngrams(sent,3) for sent in data]\n",
    "\n",
    "ngram_counts = NgramCounter(text_bigrams + text_unigrams + text_trigrams)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 22369621 ngram orders and 6857028 ngrams>\n",
      "12.698103427886963\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(ngram_counts)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
