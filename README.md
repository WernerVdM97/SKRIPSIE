# This repository includes:

## A report of the entire project

## The self-written code for:
### First Neural Network
  - Feature extraction given a file containing n-grams with their counts
  - Training of a neural network to produce a discounted probability for each n-gram
  - Rewriting these probabilities to an existing unsmoothed language model in ARPA format
  
### Second Network Network
  - Feature extraction given a file containing n-grams with their counts
  - Training of a neural network to produce a "true" MLE for each n-gram
  - Rewriting these probabilities and discounting them to an existing unsmoothed language model in ARPA format
 
## Finally scripts to apply these networks.

### Other files included are examples of basic interactions with neural networks and language modelling.
